{"cells":[{"cell_type":"markdown","metadata":{"id":"V4ZptIDhDSLY"},"source":["\n","# Datapoints FAQ\n","\n","The :mod:`torchvision.datapoints` namespace was introduced together with ``torchvision.transforms.v2``. This example\n","showcases what these datapoints are and how they behave. This is a fairly low-level topic that most users will not need\n","to worry about: you do not need to understand the internals of datapoints to efficiently rely on\n","``torchvision.transforms.v2``. It may however be useful for advanced users trying to implement their own datasets,\n","transforms, or work directly with the datapoints.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"1mLxO-0tDSLd","executionInfo":{"status":"ok","timestamp":1694566821992,"user_tz":180,"elapsed":8681,"user":{"displayName":"Allan Santos","userId":"09351383303514612345"}}},"outputs":[],"source":["import PIL.Image\n","\n","import torch\n","import torchvision\n","\n","# We are using BETA APIs, so we deactivate the associated warning, thereby acknowledging that\n","# some APIs may slightly change in the future\n","torchvision.disable_beta_transforms_warning()\n","\n","from torchvision import datapoints"]},{"cell_type":"markdown","metadata":{"id":"ADXCmIflDSLf"},"source":["## What are datapoints?\n","\n","Datapoints are zero-copy tensor subclasses:\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"l0gIpO6eDSLg","executionInfo":{"status":"ok","timestamp":1694566821993,"user_tz":180,"elapsed":6,"user":{"displayName":"Allan Santos","userId":"09351383303514612345"}}},"outputs":[],"source":["tensor = torch.rand(3, 256, 256)\n","image = datapoints.Image(tensor)\n","\n","assert isinstance(image, torch.Tensor)\n","assert image.data_ptr() == tensor.data_ptr()"]},{"cell_type":"markdown","metadata":{"id":"hLdCi5ebDSLg"},"source":["Under the hood, they are needed in :mod:`torchvision.transforms.v2` to correctly dispatch to the appropriate function\n","for the input data.\n","\n","## What datapoints are supported?\n","\n","So far :mod:`torchvision.datapoints` supports four types of datapoints:\n","\n","* :class:`~torchvision.datapoints.Image`\n","* :class:`~torchvision.datapoints.Video`\n","* :class:`~torchvision.datapoints.BoundingBox`\n","* :class:`~torchvision.datapoints.Mask`\n","\n","## How do I construct a datapoint?\n","\n","Each datapoint class takes any tensor-like data that can be turned into a :class:`~torch.Tensor`\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_SICu12oDSLh","executionInfo":{"status":"ok","timestamp":1694566822449,"user_tz":180,"elapsed":461,"user":{"displayName":"Allan Santos","userId":"09351383303514612345"}},"outputId":"25d5c2c0-b0fd-4caa-dcf7-9899aa9a99b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Image([[[[0, 1],\n","         [1, 0]]]], )\n"]}],"source":["image = datapoints.Image([[[[0, 1], [1, 0]]]])\n","print(image)"]},{"cell_type":"markdown","metadata":{"id":"eztfz0fiDSLi"},"source":["Similar to other PyTorch creations ops, the constructor also takes the ``dtype``, ``device``, and ``requires_grad``\n","parameters.\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KWnievXZDSLi","executionInfo":{"status":"ok","timestamp":1694566822450,"user_tz":180,"elapsed":12,"user":{"displayName":"Allan Santos","userId":"09351383303514612345"}},"outputId":"93923ecd-039f-4f4c-fdc3-02d0a86b64f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Image([[[0., 1.],\n","        [1., 0.]]], grad_fn=<AliasBackward0>, )\n"]}],"source":["float_image = datapoints.Image([[[0, 1], [1, 0]]], dtype=torch.float32, requires_grad=True)\n","print(float_image)"]},{"cell_type":"markdown","metadata":{"id":"fiUxyBv8DSLj"},"source":["In addition, :class:`~torchvision.datapoints.Image` and :class:`~torchvision.datapoints.Mask` also take a\n",":class:`PIL.Image.Image` directly:\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":314},"id":"QbEZxOlfDSLj","executionInfo":{"status":"error","timestamp":1694566822450,"user_tz":180,"elapsed":10,"user":{"displayName":"Allan Santos","userId":"09351383303514612345"}},"outputId":"dc4c53ab-4c67-40cc-886b-e1ff2f929ee5"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-70ea12334a8a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatapoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"assets/astronaut.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'assets/astronaut.jpg'"]}],"source":["image = datapoints.Image(PIL.Image.open(\"assets/astronaut.jpg\"))\n","print(image.shape, image.dtype)"]},{"cell_type":"markdown","metadata":{"id":"cUEHRpTjDSLk"},"source":["In general, the datapoints can also store additional metadata that complements the underlying tensor. For example,\n",":class:`~torchvision.datapoints.BoundingBox` stores the coordinate format as well as the spatial size of the\n","corresponding image alongside the actual values:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bT6-MSqcDSLk","executionInfo":{"status":"aborted","timestamp":1694566822451,"user_tz":180,"elapsed":8,"user":{"displayName":"Allan Santos","userId":"09351383303514612345"}}},"outputs":[],"source":["bounding_box = datapoints.BoundingBox(\n","    [17, 16, 344, 495], format=datapoints.BoundingBoxFormat.XYXY, spatial_size=image.shape[-2:]\n",")\n","print(bounding_box)"]},{"cell_type":"markdown","metadata":{"id":"6nFsx7MLDSLk"},"source":["## Do I have to wrap the output of the datasets myself?\n","\n","Only if you are using custom datasets. For the built-in ones, you can use\n",":func:`torchvision.datasets.wrap_dataset_for_transforms_v2`. Note that the function also supports subclasses of the\n","built-in datasets. Meaning, if your custom dataset subclasses from a built-in one and the output type is the same, you\n","also don't have to wrap manually.\n","\n","## How do the datapoints behave inside a computation?\n","\n","Datapoints look and feel just like regular tensors. Everything that is supported on a plain :class:`torch.Tensor`\n","also works on datapoints.\n","Since for most operations involving datapoints, it cannot be safely inferred whether the result should retain the\n","datapoint type, we choose to return a plain tensor instead of a datapoint (this might change, see note below):\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UtXqXEzBDSLk","executionInfo":{"status":"aborted","timestamp":1694566822451,"user_tz":180,"elapsed":8,"user":{"displayName":"Allan Santos","userId":"09351383303514612345"}}},"outputs":[],"source":["assert isinstance(image, datapoints.Image)\n","\n","new_image = image + 0\n","\n","assert isinstance(new_image, torch.Tensor) and not isinstance(new_image, datapoints.Image)"]},{"cell_type":"markdown","metadata":{"id":"7-0Hs0oiDSLl"},"source":["<div class=\"alert alert-info\"><h4>Note</h4><p>This \"unwrapping\" behaviour is something we're actively seeking feedback on. If you find this surprising or if you\n","   have any suggestions on how to better support your use-cases, please reach out to us via this issue:\n","   https://github.com/pytorch/vision/issues/7319</p></div>\n","\n","There are two exceptions to this rule:\n","\n","1. The operations :meth:`~torch.Tensor.clone`, :meth:`~torch.Tensor.to`, and :meth:`~torch.Tensor.requires_grad_`\n","   retain the datapoint type.\n","2. Inplace operations on datapoints cannot change the type of the datapoint they are called on. However, if you use\n","   the flow style, the returned value will be unwrapped:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qyRpAaItDSLl","executionInfo":{"status":"aborted","timestamp":1694566822452,"user_tz":180,"elapsed":9,"user":{"displayName":"Allan Santos","userId":"09351383303514612345"}}},"outputs":[],"source":["image = datapoints.Image([[[0, 1], [1, 0]]])\n","\n","new_image = image.add_(1).mul_(2)\n","\n","assert isinstance(image, torch.Tensor)\n","print(image)\n","\n","assert isinstance(new_image, torch.Tensor) and not isinstance(new_image, datapoints.Image)\n","assert (new_image == image).all()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}